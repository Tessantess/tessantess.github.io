---
layout:     post
title:      Linear Coupling
subtitle:    
date:       2019-04-22
author:     Tintin
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - optimization
---
### 梯度下降与镜像下降的线性偶和

#### 导语

一阶算法在大型机器学习中扮演一个很重要的角色。尽管对于一些特定的问题有很多基于方差方法，但是这些方法都是基于两种算法：梯度下降针对原始问题，针对对偶问题的镜像下降。  
我们发现梯度下降和镜像下降是互补的，因此可以通过将两者线性耦合设计一个更快速的算法。我们如何用线性偶和构造Nesterov加速梯度算法，并给出了一个比Nesterov原证明更加清晰的解释。并讨论了将其扩展到Nesterov不能应用的场景下。

### 1. 原始问题：梯度下降

假设$$f(x)$$满足$$L-smooth$$,可以得到([证明见这里](https://tintin.space/2019/04/09/Proximal-Descent/))：

$$x_{k+1} = x_k -\frac{1}{L}\triangledown f(x_k)$$

且([证明见这里](https://tintin.space/2019/03/29/Convergence/)):

$$f(x_k)-f(x_{k+1})\geq \frac{1}{2L}||\triangledown f(x_k)||^2$$

$$GD$$的缺陷是它没有对最优点$$f(x^*)$$构造一个更好的下界，并忽略了对偶问题，下面我们介绍对偶问题中的镜像下降。

### 2. 对偶问题：镜像下降

梯度序列$$\triangledown f(x_k)$$可以看成是目标函数$$f$$的下降超平面：

$$f(\mu)\geq f(x)+ \left \langle \triangledown f(x),\mu-x  \right \rangle$$

镜像下降算法尝试构造这些超平面的凸集从而产生一个更强的下界。假设有点序列$$x_0,...,x_{k-1}$$,我们可以表示$$k$$个超平面的线性组合：

$$f(\mu)\geq \frac{1}{k}\sum_{t=0}^{k-1}f(x_t)+ \frac{1}{k}\sum_{t=0}^{k-1}\left \langle \triangledown f(x_t),\mu-x_t  \right \rangle$$

而根据琴声不等式有：

$$f(\overline{x})\leq  \frac{1}{k}\sum_{t=0}^{k-1}f(x_t)$$

$$\therefore f(\overline{x})-f(\mu)\leq \frac{1}{k}\sum_{t=0}^{k-1}\left \langle \triangledown f(x_t),x_t-\mu  \right \rangle \equiv R_k(\mu) $$

我们将$$R_k(\mu)$$叫做$$Regret$$,并考虑正则化的$$Regret, \widetilde{R}_k(\mu)$$:

$$ \widetilde{R}_k(\mu)= \frac{1}{k}(-\frac{1}{\alpha}w(\mu) + \sum_{t=0}^{k-1}\left \langle \triangledown f(x_t),x_t-\mu  \right \rangle) $$

其中$$\alpha>0$$是正则系数，$$w(\mu)$$是正则项且是强凸的。然后mirror descent更新:

$$x_{k} \leftarrow \max_{\mu}\widetilde{R}_k(\mu) $$

随着迭代次数$$k$$的增多,$$\widetilde{R}_k(\mu)$$会逐渐减小直到收敛，收敛速度为$$O(\frac{\rho^2}{\epsilon^2})$$,其中$$\rho$$是$$\|\triangledown f(x_k)\|^2_*$$的平均值。

对于平滑的目标函数，mirror descent的收敛速度和梯度下降类似都是$$\frac{1}{\epsilon}$$.mirror descent几个不同的实现方式如Nemirovski’s mirror和Nesterov’s dual averaging，本文采用Nemirovski’s mirror。

**Definition 2.2：** distance generating function(DGF)距离产生方程，Bregman divergence:

$$ V_x(y) = w(y) - w(x)- \left \langle w(x),y-x \right \rangle$$

它满足$$V_x(x)=0,V_x(y)\geq 0$$

**Definition 2.3:**镜像下降的更新距离与步长$$\alpha$$之间的关系为：
$$\widetilde{x}=Mirr_x(\alpha *\partial f(x)) $$

$$Mirr_x(\varepsilon )=\min_y \{V_x(y)+\left \langle \varepsilon,y-x\right \rangle  \}$$

【MirrorDescent三点性质】：
$$
\begin{split}
||x_{k+1}-x^*||^2
&=||x_{k}-\alpha \triangledown f(x_k)-x^*||^2\\
&= ||x_{k}-x^*||^2-2\alpha\triangledown f(x_k)(x_{k}-x^*)+\alpha^2||\triangledown f(x_k)||^2 \\
\end{split}
$$

$$\therefore \alpha \triangledown f(x_k)(x_{k}-x^*) = \frac{\alpha^2}{2}||\triangledown f(x_k)||^2 + V_{x_k}(x^*) -V_{x_{k+1}}(x^*)  $$

【MirrorDescent核心引理】

$$
\begin{split}
\alpha (f(x_k)-f(x^*))& \leq  \alpha \triangledown f(x_k)(x_{k}-x^*) \\
&  \leq   \frac{\alpha^2}{2}||\triangledown f(x_k)||^2 + V_{x_k}(x^*) -V_{x_{k+1}}(x^*) 
\end{split}
$$

【T步骤求和】

$$\begin{split}
左边：\sum_{k=0}^{T-1}\alpha (f(x_k)-f(x^*))& = \alpha \sum_{k=0}^{T-1} f(x_k)-\alpha T f(x^*) \\
& \geq \alpha T f( \overline{x})-\alpha T f(x^*)  【琴生不等式】\\
& \geq  \alpha T(f( \overline{x})-f(x^*)  )
\end{split}
$$

$$\begin{split}
右边：\alpha T(f( \overline{x})-f(x^*)) &\leq  \frac{\alpha^2}{2}\sum_{k=0}^{T-1} ||\triangledown f(x_k)||^2 + V_{x_0}(x^*) -V_{x_{T}}(x^*)  \\
& \leq  \frac{\alpha^2}{2}\sum_{k=0}^{T-1} ||\triangledown f(x_k)||^2 + V_{x_0}(x^*)
\end{split}
$$

令$$\rho = \frac{1}{T}\sum_{k=0}^{T-1}\|\triangledown f(x_k)\|^2$$为梯度平方均值，常数$$\Theta =V_{x_0}(x^*)$$为起点到最优点距离,那么：

$$\begin{split}
\alpha T(f( \overline{x})-f(x^*)) \leq \frac{\alpha^2}{2} T \rho^2 + \Theta 
\end{split}
$$

令$$\alpha = \frac{\sqrt{2\Theta}}{\rho \sqrt{T}}$$,那么：

$$f( \overline{x})-f(x^*) \leq \frac{2\Theta}{\alpha T} = \frac{\sqrt{2\Theta}· \rho}{ \sqrt{T}}  $$

达到$$\epsilon$$精度，需要:

$$T \geq \frac{2\Theta· \rho^2}{\epsilon^2 }  $$

在非光滑的情况下，$$MD$$的收敛速度是$$O(\frac{1}{\epsilon^2})$$，比$$GD$$慢一个量级，而在光滑的情况下两者类似。


### 概念问题
我们从高层次角度来描述梯度下降和镜像下降：梯度下降主要用于原始问题，当梯度$$ \|  \triangledown f(x_k) \|$$很大时，它采用局部步长并取得快速收敛，在函数比较陡峭的时候梯度下降下降的也多，而比较平缓的地方梯度更新的也少。相反，镜像下降用于对偶问题，当$$\|\triangledown f(x_k)\|$$很小时，它采用全局步长来达到快速收敛。

<font color='red'>可以结合梯度下降和镜像下降达到一个更快速的一阶算法吗？</font>

本文提出了一个线性耦合框架，为了更好地讨论我们主要针对凸且光滑的场景，并展示如何用linear coupling构建Nesterov加速梯度算法,并讨论将其扩展到不适用Nesterov的其它场景下。

### 3. Linear Coupling

$$x_{k+1}\leftarrow \tau_k MD(x_{k})+(1-\tau_k)GD(x_{k})$$

其中$$\alpha_t = \frac{t+1}{2L},\tau_k = \frac{2}{t+2}$$.

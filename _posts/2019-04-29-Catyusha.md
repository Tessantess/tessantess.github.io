---
layout:     post
title:      Katyusha
subtitle:    
date:       2019-04-29
author:     Tintin
header-img: img/post-bg-swift.jpg
catalog: true
tags:
    - Optimization
---
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>


| :------ |:------| 
| $$v_0 = 0$$|     $$x_0$$      |
|  $$v_1 =  g_0$$      |    $$x_1 = x_0 - g_0$$  |
| $$v_2  = \gamma  g_0 +  g_1$$  |  $$x_2 = x_0 - (1+\gamma)g_0 - g_1 $$ |
| $$v_3  = \gamma^2  g_0 +  \gamma g_1 + g_2$$  |  $$x_3 = x_0 - (1+\gamma+\gamma^2)g_0 - (1+\gamma)g_1 - g_2 $$ |
|$$...$$|$$...$$|
| $$v_{k+1}  = \sum_{i=0}^{k} \gamma^i g_{k-i} $$  |  $$x_{k+1} = x_0 -\sum_{i=0}^{k} v_i  $$ |


| :------ | 
| $$v_0 = 0$$|   
|  $$v_1 =  g_0$$      |   
| $$v_2  = \gamma  g_0 +  g_1$$  |  
| $$v_3  = \gamma^2  g_0 +  \gamma g_1 + g_2$$  |  
|$$v_{k+1} = g_k + \gamma g_{k-1} + \gamma^2 g_{k-2} + ...+\gamma^2 g_0 $$|

## 引言

在$$SGD$$的场景下，Nesterov's momentum很难达到一般的加速算法收敛效果，这是因为$SGD$的gradient方向带有噪声，它在前期与准确的负梯度方向有一定的角度偏离，这会导致momentum进一步偏离，并导致后面每一步的更新都越来越不准确。针对凸和finit-sum问题，Katyusha在$$SGD$$的基础上达到了和一般加速算法相同的收敛速度。

|        |variance reduction |acceleration(momentum)|
| :------: |:------:  | :------: |  
|<br>$$SGD$$|$$SAG$$ <br>$$SVRG$$<br>$$SAGA$$|$$APPA$$<br>$$Catalyst$$<br>$$Catyusha$$| 

##### SGD

计算单个样本的梯度$$\triangledown_i f(x)$$比计算所有样本梯度快$$n$$倍，$$SGD$$在每次迭代中的计算损耗要比$$GD$$低，但是它的最优收敛速度为$$\frac{1}{\epsilon}$$,尽管$$F(x)$$是强凸且光滑的。

##### Varience Reduction

$$SVRG$$在凸且光滑的情况下的收敛速度为$$O((n+k)log\frac{1}{\epsilon}),k=\frac{L}{\sigma}$$,$$SVRG$$的收敛速度很大程度上取决于$$k$$的大小，目前的open question是如何设计加速随机梯度方法$$\sqrt{k}$$

##### Acceleration

$$APPA$$和$$Catlyst$$加速方法针对上述问题做了一定的提升，达到了$$O((n+\sqrt{nk})\log k\log\frac{1}{\epsilon})$$,然而它仍旧不完美：


* 最有性：它不是$$\sqrt{k}$$并且有额外的$$\log k$$；在目标函数是非强凸或者不光滑的情况下，它是次最优$$\frac{\log^4T}{T^2}$$;在目标函数非强凸且不光滑的情况下，收敛速度为$$\frac{\log^4T}{T}$$

* 实用性：在它的内部循环中
* 平行性（分布式）
* 泛化性

||强凸光滑|强凸不光滑|非强凸光滑|非强凸不光滑|
| :------: |:------:| :------: |  :------: | :------: |   
|$$SGD$$| $$O(\frac{L^2}{\sigma \epsilon})$$ | $$O(\frac{G}{\sigma \epsilon})$$ | $$O(\frac{L^2}{\epsilon^2})$$ | $$O(\frac{G}{ \epsilon^2})$$ |
|$$SVRG$$| $$O((n+k)\log\frac{1}{\epsilon})$$ | $$O(n\log\frac{1}{\epsilon}+\frac{G}{\sigma\epsilon})$$|  $$O(n\log\frac{1}{\epsilon}+\frac{L}{\epsilon})$$ ||
|$$Katyusha$$| $$O((n+\sqrt{nk})\log\frac{1}{\epsilon})$$ | | $$O(n\log\frac{1}{\epsilon}+ \sqrt{n\frac{L}{\epsilon})}$$ ||

### 2. Acceleration

##### Accelerated gradient descent

![](/img/Katyusha/momentum.png)

[这里](http://awibisono.github.io/2016/06/20/accelerated-gradient-descent.html)

| $$GD$$ | $$AGD$$ |
| :------: |:------:|  
|$$x_{k+1}=x_k-\triangledown f(x_k)$$ | $$y_k= x_k+\frac{\tau -1}{\tau+1 }(x_k-x_{k-1})$$<br>$$x_{k+1}=y_k-\triangledown f(y_k)$$|
|$$O(\frac{1}{\epsilon})$$|$$O(\frac{1}{\sqrt{\epsilon}})$$|

$$f(x)$$是convex,然而在现实生活中，$$f(x)$$是非凸的，且是finit-sum问题一般用$$SGD$$

##### Momentum

$$v_{k+1} = \gamma v_{k} +\eta\triangledown f(x_k)$$

$$x_{k+1} = x_k - v_{k+1}$$

当这一次的梯度与上一次梯度方向相同即同号时，梯度更新的距离会变大，方向不相同则变小，这就是momentum的作用，梯度方向不变用惯性增加距离加速前进，方向变化则减小震荡。

##### Nesterov's Momentum

$$v_{k+1} = \gamma v_{k} +\eta\triangledown f(x_k-\gamma v_{k})$$

$$x_{k+1} = x_k - v_{k+1}$$

与momentum不一样的是，Nesterov方法用的是下一步的梯度而不是当前的梯度.

从$$x_1$$到$$x_2$$，第一步，momentum和Nesterov先从$$x_1$$到蓝色点$$y_2$$,其距离为$$\gamma v_1$$；第二步momentum用$$\triangledown f(x_1)$$方向相同继续向前，Nesterov用$$\triangledown f(x_2)$$方向也相同继续向前跨一步。两者类似，但在靠近极值时Nesterov则表现出了明显的优势。

从$$x_2$$到$$x_3$$，同上momentum和Nesterov跨过$$\gamma v_2$$到$$y_3$$,而这次由于靠近极值点一不小心就跨过去了，这时momentum用的$$x_2$$点的梯度它会继续往右走产生更大震荡，而Nesterov用$$y_3$$的梯度与之前的梯度方向相反它会往左走达到$$x_3$$从而抑制震荡。

Nesterov预知下一步的梯度更加准确地进行加速和抑制。

![](/img/Katyusha/Nesterov3.png)

但是对于$$SGD$$来说，其梯度本身存在一定的噪声不够准确，使用Nesterov momentum会加速其偏离原本正确的方向,于是有了$$Katyusha$$.

